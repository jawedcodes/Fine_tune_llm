# -*- coding: utf-8 -*-
"""fine_tune_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XaPZENqMLlCXLWeNNRjAWF0K4Rpw3XQP
"""

!pip install -U transformers datasets accelerate evaluate torch

import os
from dataclasses import dataclass
from typing import Dict


import torch
from datasets import load_dataset
from transformers import (
AutoTokenizer,
AutoModelForCausalLM,
DataCollatorForLanguageModeling,
Trainer,
TrainingArguments,
pipeline
)

MODEL_NAME = "gpt2" # small model for learning; replace with a larger model if you have GPU/RAM
DATASET_NAME = "wikitext"
DATASET_CONFIG = "wikitext-2-raw-v1" # raw text version (no sentencepiece tokenization)
OUTPUT_DIR = "./gpt2-finetuned-wikitext"
BATCH_SIZE = 4 # per device; lower if you run out of memory
GRAD_ACCUMULATION_STEPS = 8 # to simulate larger batch sizes
EPOCHS = 2 # increase for real training
LEARNING_RATE = 5e-5
MAX_SEQ_LENGTH = 512 # collapse/trim text to this length for each example
SEED = 42

def set_seed(seed: int = SEED):
  import random


  random.seed(seed)
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)


set_seed()

print("Loading dataset...")
dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)
print(dataset)

dataset['train'].data

dataset['train'].select(range(2000))

small_train = dataset["train"].select(range(2000)) if len(dataset["train"]) > 2000 else dataset["train"]
small_valid = dataset["validation"].select(range(500)) if len(dataset["validation"]) > 500 else dataset["validation"]

print("Loading tokenizer and model...")
# GPT-2 tokenizer doesn't have a padding token by default; we will add one (required for batching).
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
if tokenizer.pad_token is None:
  tokenizer.add_special_tokens({"pad_token": "<|pad|>"})


model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
# Resize token embeddings if tokenizer changed (because we added pad_token)
model.resize_token_embeddings(len(tokenizer))

# 3) Preprocessing: tokenize and group texts into blocks
# -----------------------------
print("Tokenizing and grouping texts...")


# Tokenize function: we will tokenize and return input_ids


def tokenize_function(examples: Dict[str, list]):
  return tokenizer(examples["text"], return_special_tokens_mask=False)


train_tokenized = small_train.map(
  tokenize_function,
  batched=True,
  remove_columns=[c for c in small_train.column_names if c != "text"],
)
valid_tokenized = small_valid.map(
  tokenize_function,
  batched=True,
  remove_columns=[c for c in small_valid.column_names if c != "text"],
)

def group_texts(examples: Dict[str, list]):
  # Concatenate all text input_ids
  concatenated = sum(examples["input_ids"], [])
  total_length = len(concatenated)
  # Drop the small remainder to make lengths divisible by block_size
  block_size = MAX_SEQ_LENGTH
  if total_length >= block_size:
    total_length = (total_length // block_size) * block_size
  else:
    total_length = 0
  result = {"input_ids": [], "attention_mask": [], "labels": []}
  for i in range(0, total_length, block_size):
    chunk = concatenated[i : i + block_size]
    result["input_ids"].append(chunk)
    result["attention_mask"].append([1] * block_size)
    result["labels"].append(chunk.copy())
  return result


train_dataset = train_tokenized.map(
    group_texts,
    batched=True,
    batch_size=-1, # Process the entire dataset as one batch
    num_proc=1,
    remove_columns=train_tokenized.column_names, # Remove original columns after grouping
)
valid_dataset = valid_tokenized.map(
    group_texts,
    batched=True,
    batch_size=-1, # Process the entire dataset as one batch
    num_proc=1,
    remove_columns=valid_tokenized.column_names, # Remove original columns after grouping
)


print("Train dataset size (examples):", len(train_dataset))
print("Validation dataset size (examples):", len(valid_dataset))

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# 5) TrainingArguments & Trainer
# -----------------------------
training_args = TrainingArguments(
output_dir=OUTPUT_DIR,
overwrite_output_dir=True,
num_train_epochs=EPOCHS,
per_device_train_batch_size=BATCH_SIZE,
per_device_eval_batch_size=BATCH_SIZE,
gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,
eval_strategy="epoch",
save_strategy="epoch",
learning_rate=LEARNING_RATE,
weight_decay=0.01,
fp16=torch.cuda.is_available(), # mixed precision if GPU available
push_to_hub=False,
logging_dir=OUTPUT_DIR + "/logs",
logging_steps=100,
report_to=[],
)

trainer = Trainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=valid_dataset,
data_collator=data_collator,
)

# -----------------------------
print("Starting training...")
trainer.train()

# -----------------------------
print("Saving model to", OUTPUT_DIR)
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("Running quick generation test...")
# load from saved dir to simulate real usage
from transformers import AutoModelForCausalLM, AutoTokenizer


model2 = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR)
tokenizer2 = AutoTokenizer.from_pretrained(OUTPUT_DIR)


generator = pipeline("text-generation", model=model2, tokenizer=tokenizer2, device=0 if torch.cuda.is_available() else -1)
prompt = "Artificial intelligence in the future will"
gen = generator(prompt, max_length=100, num_return_sequences=1)
print("\nGeneration result:\n", gen[0]["generated_text"])

