# -*- coding: utf-8 -*-
"""Full_Fine_Tuneipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cqw-6l0UJ8N9v_XLBSAQy2lDBY21I1Ui
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers datasets torch accelerate scikit-learn

# Define model and dataset names
model_checkpoint = "distilbert-base-uncased"
dataset_name = "imdb"

from datasets import load_dataset

# Load the dataset
raw_datasets = load_dataset(dataset_name)

# Display dataset structure (optional)
print(raw_datasets)
# DatasetDict({
#     train: Dataset({
#         features: ['text', 'label'],
#         num_rows: 25000
#     })
#     test: Dataset({
#         features: ['text', 'label'],
#         num_rows: 25000
#     })
#     unsupervised: Dataset({
#         features: ['text', 'label'],
#         num_rows: 50000
#     })
# })

raw_datasets.data['test'].shape

raw_datasets.data

from transformers import AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Define the tokenization function
def tokenize_function(examples):
    # Truncate sequences longer than the model's max input size
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

# Apply tokenization to the entire dataset (batched for efficiency)
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# Remove the original 'text' column as it's no longer needed
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
# Rename 'label' to 'labels' which is expected by the Trainer
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
# Set the format to PyTorch tensors
tokenized_datasets.set_format("torch")

# Create smaller subsets for quicker demonstration (optional)
# Remove or adjust these lines for a full run
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

print("\nSample tokenized training data:")
print(small_train_dataset[0])

from transformers import AutoModelForSequenceClassification

# Load the model for sequence classification
# num_labels=2 for binary classification (positive/negative)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)

model.num_parameters()

from transformers import TrainingArguments

# Define output directory for checkpoints and logs
output_dir = "./results/distilbert-imdb-full"

training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch", # Evaluate performance at the end of each epoch
    save_strategy="epoch",       # Save checkpoint at the end of each epoch
    num_train_epochs=2,          # Number of training epochs (adjust as needed)
    per_device_train_batch_size=8, # Batch size per GPU
    per_device_eval_batch_size=8,  # Batch size for evaluation
    learning_rate=2e-5,          # Starting learning rate (a common value for fine-tuning)
    weight_decay=0.01,           # Apply weight decay for regularization
    logging_dir='./logs',        # Directory for storing logs
    logging_steps=100,           # Log training loss every 100 steps
    load_best_model_at_end=True, # Load the best performing model checkpoint at the end
    metric_for_best_model="accuracy", # Metric to determine the 'best' model
    # Use push_to_hub=True to upload results to Hugging Face Hub (requires login)
    # push_to_hub=False,
)

# Commented out IPython magic to ensure Python compatibility.
# %pip install evaluate

import numpy as np
from evaluate import load



# Load the accuracy metric
metric = load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset, # Use full dataset for actual training
    eval_dataset=small_eval_dataset,   # Use full test set for proper evaluation
    tokenizer=tokenizer,               # Useful for padding collation
    compute_metrics=compute_metrics,
)

"""**Training**"""

# Start the training process
train_result = trainer.train()

# Optionally, save training metrics
trainer.log_metrics("train", train_result.metrics)
trainer.save_metrics("train", train_result.metrics)

# Save the final fine-tuned model and tokenizer
trainer.save_model(output_dir) # Saves the best model due to load_best_model_at_end=True
trainer.save_state() # Saves trainer state including RNG states

# Evaluate the final model
eval_results = trainer.evaluate()

# Print evaluation results
print(f"Evaluation results: {eval_results}")
trainer.log_metrics("eval", eval_results)
trainer.save_metrics("eval", eval_results)

"""**Prediction**"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_dir = "./results/distilbert-imdb-full"

model = AutoModelForSequenceClassification.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

inputs = tokenizer("This movie is amazing!", return_tensors="pt")
outputs = model(**inputs)
pred = outputs.logits.argmax(-1).item()

print("Prediction:", pred)

inputs = tokenizer("I hate that movie", return_tensors="pt")
outputs = model(**inputs)
pred = outputs.logits.argmax(-1).item()

print("Prediction:", pred)

inputs = tokenizer("that movie was so so", return_tensors="pt")
outputs = model(**inputs)
pred = outputs.logits.argmax(-1).item()

print("Prediction:", pred)

review='''“The movie completely exceeded my expectations.
The performances were powerful, the story was emotional, and the direction was absolutely brilliant.
I was hooked from start to end, and I would definitely watch it again.”"'''

inputs = tokenizer(review, return_tensors="pt")
outputs = model(**inputs)
pred = outputs.logits.argmax(-1).item()

print("Prediction:", pred)

